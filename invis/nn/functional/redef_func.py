#!/usr/bin/env python3

import itertools
import math

import numpy as np

import megengine as mge
import megengine.functional as F
from megengine.random import normal, uniform

import invis as inv

from .helper import ensure_tensor_list, ensure_tensor_type, values_indices
from .inner_func import logical_not

__all__ = [
    "matmul", "from_numpy",
    "ones", "ones_like", "zeros", "zeros_like",
    "rand", "randn",
    "mean", "std", "var", "norm", "sum", "prod", "cumsum", "cumprod",
    "cat", "stack", "equal", "topk", "sort", "argsort",
    "transpose", "permute", "reshape", "squeeze", "unsqueeze", "flatten",
    "interpolate", "pad", "chunk", "split",
    "avg_pool2d", "max_pool2d",
    "round", "clamp", "clamp_",
    "log_softmax", "relu", "leaky_relu", "sigmoid",
    "pixel_shuffle", "pixel_unshuffle",
    "meshgrid", "linspace", "logspace", "arange",
    "nonzero", "where",
    "maximum", "minimum",
    "isnan", "isinf", "isfinite",
    "gather", "scatter",
    "tile", "repeat_interleave", "broadcast_to",
]


def equal(input, other) -> bool:
    """`True` if two tensors have the same size and elements, `False` otherwise."""
    if input.shape != other.shape:
        return False
    return not bool((input != other).sum())


@ensure_tensor_type
def from_numpy(input):
    return inv.Tensor(input)


@ensure_tensor_type
def mean(input, dim=None, keepdim=False, *, dtype=None, out=None):
    return mge.Tensor.mean(input, axis=dim, keepdims=keepdim)


@ensure_tensor_type
def var(input, dim=None, unbiased=True, keepdim=False, *, out=None):
    # megengine doesn't support unbiased=True
    factor = 1
    if unbiased:
        numel = np.prod(input.shape).item() if dim is None else input.shape[dim]
        factor = numel / (numel - 1)

    if dim is None:
        m = mean(input, dim, keepdim=keepdim)
    else:
        m = mean(input, dim, keepdim=True)

    v = input - m
    return mean(v ** 2, dim, keepdim=keepdim) * factor


@ensure_tensor_type
def std(input, dim=None, unbiased=True, keepdim=False, *, out=None):
    return var(input, dim, unbiased, keepdim) ** 0.5


@ensure_tensor_type
def norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None):
    return F.norm(input, order=p, axis=dim, keepdims=keepdim)


@ensure_tensor_type
def log_softmax(input, dim, *, dtype=None):
    return F.logsoftmax(input, axis=dim)


@ensure_tensor_type
def matmul(input, other, *, out=None):
    return F.matmul(input, other, out=out)


@ensure_tensor_type
def ones(*shape, dtype='float32', device=None):
    return F.ones(shape, dtype=dtype)


@ensure_tensor_type
def ones_like(input, *, dtype='float32', device=None):
    return ones(*input.shape, dtype=dtype)


@ensure_tensor_type
def zeros(*shape, dtype='float32', device=None):
    return F.zeros(shape, dtype=dtype)


@ensure_tensor_type
def zeros_like(input, *, dtype='float32', device=None):
    return zeros(*input.shape, dtype=dtype)


@ensure_tensor_type
def rand(*size):
    return uniform(0, 1, size)


@ensure_tensor_type
def randn(*size):
    return normal(0, 1, size)


@ensure_tensor_type
def round(input, *, decimals=0, out=None):
    if decimals > 0:
        factor = 10 ** decimals
        input = F.round(input * factor)
        return input / factor
    else:
        return F.round(input)


@ensure_tensor_type
def pixel_shuffle(input, upscale_factor):
    return F.pixel_shuffle(input, upscale_factor)


@ensure_tensor_type
def pixel_unshuffle(input, downscale_factor):
    """pixel unshuffle"""
    *other_dim, c, h, w = input.shape
    input = input.reshape(-1, c, h, w)
    c = input.shape[1]

    kernel = zeros(
        [downscale_factor * downscale_factor * c, 1, downscale_factor, downscale_factor],
    )
    for y in range(downscale_factor):
        for x in range(downscale_factor):
            kernel[x + y * downscale_factor::downscale_factor*downscale_factor, 0, y, x] = 1
    out = F.conv2d(input, kernel, stride=downscale_factor, groups=c)
    _, c, h, w = out.shape
    return out.reshape(*other_dim, c, h, w)


@ensure_tensor_type
def clamp(input, min=None, max=None, *, out=None):
    return F.clip(input, lower=min, upper=max)


@ensure_tensor_type
def clamp_(input, min=None, max=None):
    result = F.clip(input, lower=min, upper=max)
    input._reset(result)
    return input


# maybe the following function could be generated by code
@ensure_tensor_type
def relu(input, inplace=False):
    return F.relu(input)


@ensure_tensor_type
def sigmoid(input):
    return F.sigmoid(input)


@ensure_tensor_type
def leaky_relu(input, negative_slope=0.01, inplace=False):
    return F.leaky_relu(input, negative_slope=negative_slope)


@ensure_tensor_type
def cat(tensors, dim=0, *, out=None):
    cat_out = F.concat(tensors, axis=dim)
    return cat_out


@ensure_tensor_type
def stack(tensors, dim=0, *, out=None):
    return F.stack(tensors, axis=dim)


def topk(input, k, dim=None, largest=True, sorted=True, *, out=None):
    # NOTE: topk in megengine only sort last dim and 1D/2D tensor
    if dim is None:
        dim = -1
    input = transpose(input, dim, -1)
    *keep_shape, last_dim = input.shape
    input = input.reshape(-1, last_dim)

    v = F.topk(input, k, descending=largest, no_sort=not sorted)
    return values_indices(
        values=inv.Tensor(transpose(v[0].reshape(*keep_shape, -1), dim, -1)),
        indices=inv.Tensor(transpose(v[1].reshape(*keep_shape, -1), dim, -1)),
    )


@ensure_tensor_type
def sort(input, dim=-1, descending=False, *, out=None):
    # NOTE: sort in megengine only sort last dim and 1D/2D tensor
    input = transpose(input, dim, -1)
    *keep_shape, last_dim = input.shape
    input = input.reshape(-1, last_dim)

    v = F.sort(input, descending=descending)
    return type(v)((transpose(x.reshape(*keep_shape, -1), dim, -1) for x in v))


@ensure_tensor_type
def squeeze(input, dim=None, out=None):
    return F.squeeze(input, axis=dim)


@ensure_tensor_type
def unsqueeze(input, dim):
    return F.expand_dims(input, axis=dim)


@ensure_tensor_type
def transpose(input, dim0, dim1):
    if not isinstance(input, inv.Tensor):
        return inv.Tensor.transpose(input, dim0, dim1)

    return input.transpose(dim0, dim1)


@ensure_tensor_list
def chunk(input, chunks, dim=0):
    """
    all returned chunks will be the same size, except the last one.
    """
    dim_shape = input.shape[dim]
    chunk_size = math.ceil(dim_shape / chunks)
    split_list = [chunk_size for _ in range(chunks - 1)]
    split_list = list(itertools.accumulate(split_list))
    return F.split(input, split_list, axis=dim)


@ensure_tensor_list
def split(tensor, split_size_or_sections, dim=0):
    if isinstance(split_size_or_sections, (tuple, list)):
        split_size_or_sections = list(itertools.accumulate(split_size_or_sections))
        split_size_or_sections = split_size_or_sections[:-1]

    return F.split(tensor, split_size_or_sections, axis=dim)


@ensure_tensor_type
def maximum(input, other, *, out=None):
    return F.maximum(input, other)


@ensure_tensor_type
def minimum(input, other, *, out=None):
    return F.minimum(input, other)


@ensure_tensor_type
def argsort(input, dim=-1, descending=False):
    _, indices = sort(input, dim=dim, descending=descending)
    return indices


@ensure_tensor_type
def isnan(input):
    return F.isnan(input)


@ensure_tensor_type
def isinf(input):
    return F.isinf(input)


def isfinite(input):
    return logical_not(F.isinf(input) | F.isnan(input))


@ensure_tensor_type
def sum(input, dim=None, keepdim=False, *, dtype=None):
    return F.sum(input, axis=dim, keepdims=keepdim)


@ensure_tensor_type
def prod(input, dim=None, keepdim=False, *, dtype=None):
    return F.prod(input, axis=dim, keepdims=keepdim)


@ensure_tensor_type
def cumsum(input, dim, *, dtype=None, out=None):
    return F.cumsum(input, axis=dim)


@ensure_tensor_type
def cumprod(input, dim, *, dtype=None, out=None):
    ndim = input.ndim
    dim = dim if dim > 0 else dim + ndim
    num_loop = input.shape[dim]
    input = transpose(input, 0, dim)
    assert len(input) == num_loop
    cum_val = F.ones(input[0].shape)
    for i in range(num_loop):
        cum_val *= input[i]
        input[i] = cum_val
    return transpose(input, 0, dim)


@ensure_tensor_list
def nonzero(input, *, out=None, as_tuple=False):
    # NOTE: cond_take output indices is flatten indices in megengine, which is stupid!
    prev_shape = input.shape
    indices = F.cond_take(input != 0, input)[1]
    div_size = list(itertools.accumulate(reversed(prev_shape), lambda x, y: x * y))
    div_size = [1] + div_size[:-1]
    sep_indices = [(indices // s) % m for s, m in zip(reversed(div_size), prev_shape)]

    if as_tuple:
        return tuple(sep_indices)
    else:
        return stack(sep_indices, dim=1)


@ensure_tensor_type
def where(condition, x=None, y=None):
    if (x is None and y is not None) or (x is not None and y is None):
        raise ValueError("x and y must both be specified")

    if x is None and y is None:
        return nonzero(condition, as_tuple=True)
    else:
        return F.where(condition, x, y)


@ensure_tensor_type
def gather(input, dim, index, *, out=None):
    return F.gather(input, dim, index=index)


@ensure_tensor_type
def scatter(input, dim, index, src):
    return F.scatter(input, dim, index=index, source=src)


@ensure_tensor_list
def meshgrid(*tensors, indexing="xy"):
    """meshgrid wrapper for megengine"""
    assert len(tensors[0]) == 2
    x, y = tensors[0]
    assert len(x.shape) == 1
    assert len(y.shape) == 1
    mesh_shape = (y.shape[0], x.shape[0])
    mesh_x = F.broadcast_to(x, mesh_shape)
    mesh_y = F.broadcast_to(y.reshape(-1, 1), mesh_shape)
    if indexing == "ij":
        mesh_x, mesh_y = mesh_x.T, mesh_y.T
    return mesh_x, mesh_y


@ensure_tensor_type
def linspace(
    start, end, steps, *,
    out=None, dtype="float32", device=None, requires_grad=False
):
    return F.linspace(start, end, steps, dtype=dtype)


@ensure_tensor_type
def logspace(
    start, end, steps, base=10.0, *,
    out=None, dtype="float32", device=None, requires_grad=False
):
    return base ** F.linspace(start, end, steps, dtype=dtype)


@ensure_tensor_type
def arange(
    start=0, end=None, step=1, *, out=None, dtype="float32", device=None, requires_grad=False
):
    return F.arange(start, end, step, dtype=dtype)


@ensure_tensor_type
def permute(input, dims):
    return mge.Tensor.transpose(input, dims)


@ensure_tensor_type
def interpolate(
    input, size=None, scale_factor=None, mode="nearest", align_corners=None,
    recompute_scale_factor=None, antialias=False,
):
    return F.nn.interpolate(
        input, size=size, scale_factor=scale_factor,
        mode=mode, align_corners=align_corners
    )


def _calculate_pad_size(h, w, padding, ks, stride):
    out_h = math.ceil((h + 2 * padding - ks) / stride + 1)
    out_w = math.ceil((w + 2 * padding - ks) / stride + 1)
    pad_h = (out_h - 1) * stride + ks - 2 * padding - h
    pad_w = (out_w - 1) * stride + ks - 2 * padding - w
    return pad_h, pad_w


@ensure_tensor_type
def avg_pool2d(
    input, kernel_size, stride=None, padding=0, ceil_mode=False,
    count_include_pad=True, divisor_override=None
):
    mode = "average" if count_include_pad else "average_count_exclude_padding"
    # NOTE: megengine doesn't support ceil mode
    if ceil_mode:
        raise NotImplementedError("ceil_mode is not supported")

    return F.avg_pool2d(input, kernel_size, stride, padding, mode)


@ensure_tensor_type
def max_pool2d(
    input, kernel_size, stride=None, padding=0, dilation=1,
    ceil_mode=False, return_indices=False,
):
    # NOTE: megengine doesn't support ceil mode
    if ceil_mode:
        *_, h, w = input.shape
        pad_h, pad_w = _calculate_pad_size(h, w, padding, kernel_size, stride)
        pad_width = ((0, 0), (0, 0), (0, pad_h), (0, pad_w))
        input = F.pad(input, pad_width, constant_value=float("-inf"))

    return F.max_pool2d(input, kernel_size, stride, padding)


@ensure_tensor_type
def pad(input, pad, mode="constant", value=0):
    # e.g. pad in torch (l, t) to ((0, 0), (l, t)) in mge if dim == 2
    dim = input.ndim
    mge_pad = [(0, 0) for _ in range(dim)]
    indices = [slice(None, None) for _ in range(dim)]
    pad_dim = len(pad) // 2

    for i in range(pad_dim):
        left_pad, right_pad = pad[i * 2], pad[i * 2 + 1]
        left_slice, right_slice = None, None
        if left_pad < 0:
            left_slice = abs(left_pad)
            left_pad = 0
        if right_pad < 0:
            right_slice = right_pad
            right_pad = 0

        mge_pad[dim - 1 - i] = (left_pad, right_pad)
        indices[dim - 1 - i] = slice(left_slice, right_slice)

    input = input[tuple(indices)]
    return F.nn.pad(input, mge_pad, mode, value)


@ensure_tensor_type
def reshape(input, shape):
    return mge.Tensor.reshape(input, *shape)


@ensure_tensor_type
def flatten(input, start_dim=0, end_dim=-1):
    shape = input.shape
    dim = input.ndim
    if start_dim < 0:
        start_dim += dim
    if end_dim < 0:
        end_dim += dim + 1
    assert start_dim >= 0 and start_dim <= end_dim
    front_dim = shape[:start_dim]
    back_dim = shape[end_dim:]
    new_shape = (*front_dim, -1, *back_dim)
    return input.reshape(new_shape)


@ensure_tensor_type
def repeat_interleave(input, repeats, dim=None, *, output_size=None):
    if isinstance(repeats, mge.Tensor):
        assert dim is not None, "repeats must have the same size as input along dim"
        repeats = repeats.tolist()
        num_loops = input.shape[dim]
        chunks = list(chunk(input, num_loops, dim))
        for i in range(num_loops):
            chunks[i] = F.repeat(chunks[i], repeats[i], axis=dim)
        return F.concat(chunks, dim)

    if dim is not None and dim < 0:
        dim += input.ndim

    return F.repeat(input, repeats, axis=dim)


@ensure_tensor_type
def tile(input, dims):
    dims_len = len(dims)
    ndim = input.ndim
    if dims_len < ndim:
        dims = [1 for _ in range(ndim - dims_len)] + list(dims)
    elif dims_len > ndim:
        squeeze_len = [1 for _ in range(dims_len - ndim)]
        input = input.reshape(*squeeze_len, *input.shape)

    return F.tile(input, tuple(dims))


@ensure_tensor_type
def broadcast_to(input, shape):
    input_shape = input.shape
    if len(shape) < input.ndim:
        raise RuntimeError(
            "the number of sizes provided must be greater or \
            equal to the number of dimensions in the tensor"
        )
    else:
        input_shape = tuple(1 for _ in range(len(shape) - input.ndim)) + input_shape

    tile_dims = []
    for cur, target in zip(input_shape, shape):
        if target == -1 or cur == target:  # -1 means keep current dim
            tile_dims.append(1)
            continue
        if cur != target:
            if cur != 1:
                raise RuntimeError(
                    f"The expanded size of the tensor must match \
                    the existing size at non-singleton dimension 0.\
                    Target sizes: {shape}.  Tensor sizes: {input.shape}"
                )
            else:
                tile_dims.append(target)

    return tile(input, tuple(tile_dims))
